% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prediction_model.R
\name{mldpEHR.disease_multi_age_predictors}
\alias{mldpEHR.disease_multi_age_predictors}
\title{build an xgboost cross validation classification model with k-fold cross-validation for each featureset provided, assumed that the classification is defined by the previous model}
\usage{
mldpEHR.disease_multi_age_predictors(
  patients,
  features,
  step,
  nfolds,
  required_conditions = "id==id",
  xgboost_params = list(booster = "gbtree", objective = "binary:logistic", subsample =
    0.7, max_depth = 3, colsample_bytree = 1, eta = 0.05, min_child_weight = 1, gamma =
    0, eval_metric = "auc"),
  nrounds = 1000
)
}
\arguments{
\item{patients}{\itemize{
\item list of data.frames of all the patients in the system going back in time. For example the first data.frame represents age 80, next is 75 and so forth.
Each patient data.frame contains the following columns:
\item patient id
\item sex
\item age
\item death - age at death, NA if unknown
\item disease - age at disease, NA if unknown
\item followup - available followup time (in years) for this patient - time until end of database or until patient exists the system (not due to death)
and any additional columns required for patient filtering in the future
}}

\item{features}{\itemize{
\item list of data.frames of features
}}

\item{step}{\itemize{
\item time between prediction models
}}

\item{nfolds}{\itemize{
\item number of folds used for k-fold cross validation
}}

\item{required_conditions}{\itemize{
\item any filter to apply to the features to filter out training/testing samples (e.g. missing data)
}}

\item{xgboost_params}{\itemize{
\item parameters used for xgboost model training
}}

\item{nrounds}{\itemize{
\item number of training rounds
}}
}
\value{
the full list of predictors, according to provided patients, Each predictor is a list with the following members:
\itemize{
\item model - list of xgboost models, for each fold
\item train - data.frame containing the patients id, fold, target class and predicted value in training (each id was used in nfolds-1 for training)
\item test - data.frame containing the patients id, fold, target class and predicted value in testing (each id was tested once in the fold it was not used for training)
\item xgboost_params - the set of parameters used in xgboost
\item nrounds - number of training iterations conducted
}
}
\description{
build an xgboost cross validation classification model with k-fold cross-validation for each featureset provided, assumed that the classification is defined by the previous model
}
\examples{

library(dplyr)
library(ggplot2)
# build base predictor
N <- 1000
patients <- purrr::map(0:5, ~ data.frame(
    id = 1:N,
    sex = rep(1, N),
    age = 80 - .x * 5,
    death = c(rep(NA, 0.4 * N), rep(82, 0.6 * N)),
    disease = rep(rep(c(NA, 81), each = N / 4), 2),
    followup = .x * 5 + 5
)) \%>\%
    setNames(seq(80, by = -5, length.out = 6))
features <- purrr::map(0:5, ~ data.frame(
    id = 1:N,
    a = c(rnorm(0.4 * N), rnorm(0.6 * N, mean = 2, sd = 1)),
    b = rep(c(rnorm(N / 4), rnorm(N / 4, mean = 3)), 2)
)) \%>\% setNames(seq(80, by = -5, length.out = 6))
predictors <- mldpEHR.disease_multi_age_predictors(patients, features, 5, 3)


test <- purrr::map2_df(predictors, names(predictors), ~ .x$test \%>\%
    mutate(n = .y) \%>\%
    arrange(id) \%>\%
    mutate(
        outcome =
            c(
                rep("healthy", 0.25 * N),
                rep("disease", 0.15 * N),
                rep("disease_death", 0.1 * N),
                rep("death", 0.25 * N),
                rep("disease_death", 0.25 * N)
            )
    ))
ggplot(test, aes(x = predict, colour = factor(outcome))) +
    facet_wrap(~n, nrow = 1) +
    stat_ecdf() +
    theme_bw()

}
